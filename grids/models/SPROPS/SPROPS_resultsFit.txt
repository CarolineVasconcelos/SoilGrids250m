Results of model fitting 'randomForest / XGBoost':


Variable: ORCDRC
Ranger result

Call:
 ranger(formulaString.lst[[j]], data = dfs, importance = "impurity",      write.forest = TRUE, mtry = t.mrfX$bestTune$mtry) 

Type:                             Regression 
Number of trees:                  500 
Sample size:                      605054 
Number of independent variables:  157 
Mtry:                             40 
Target node size:                 5 
Variable importance mode:         impurity 
OOB prediction error:             1020.754 
R squared:                        0.6539751 

 Variable importance:
             [,1]
DEPTH.f 303482207
T10MOD3  49199791
T09MOD3  37123742
T04MOD3  28086719
M03MOD4  26651559
TMDMOD3  24771562
TWIMRG5  24758707
M11MOD4  22095703
DEMMRG5  21477499
VBFMRG5  21085036
M12MOD4  18767705
M02MOD4  17342579
I10MOD4  16393707
I09MOD4  16142556
T03MOD3  15669119

eXtreme Gradient Boosting 

605054 samples
   157 predictor

No pre-processing
Resampling: Cross-Validated (3 fold, repeated 1 times) 
Summary of sample sizes: 403369, 403369, 403370 
Resampling results across tuning parameters:

  eta  max_depth  nrounds  RMSE      Rsquared 
  0.3  2           50      44.95605  0.3171805
  0.3  2          100      44.06520  0.3436179
  0.3  3           50      43.00347  0.3752022
  0.3  3          100      42.03351  0.4028562
  0.4  2           50      44.59545  0.3270205
  0.4  2          100      43.78939  0.3510342
  0.4  3           50      42.74719  0.3822094
  0.4  3          100      41.60070  0.4146484

Tuning parameter 'gamma' was held constant at a value of 0
Tuning
 parameter 'colsample_bytree' was held constant at a value of 0.8

Tuning parameter 'min_child_weight' was held constant at a value of 1
RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nrounds = 100, max_depth = 3, eta
 = 0.4, gamma = 0, colsample_bytree = 0.8 and min_child_weight = 1. 

 XGBoost variable importance:
    Feature        Gain       Cover   Frequence
 1: DEPTH.f 0.311076025 0.065332254 0.126822157
 2: T09MOD3 0.115060133 0.014256447 0.007288630
 3: T03MOD3 0.068976990 0.015847550 0.011661808
 4: M03MOD4 0.056171066 0.016349280 0.010204082
 5: I10MOD4 0.025838123 0.004048312 0.008746356
 6: TWIMRG5 0.025517500 0.024060176 0.029154519
 7: M11MOD4 0.024865189 0.011555799 0.005830904
 8: T04MOD3 0.023352884 0.003433574 0.004373178
 9: T01MOD3 0.015232377 0.006888633 0.008746356
10: I11MOD4 0.013083081 0.017228987 0.011661808
11: P03MRG3 0.012800143 0.000604062 0.007288630
12: M02MOD4 0.012652028 0.013137984 0.013119534
13: TMDMOD3 0.011806369 0.004347162 0.002915452
14: VBFMRG5 0.011420108 0.006766412 0.021865889
15: DEMMRG5 0.009905319 0.029339018 0.030612245
--------------------------------------

Variable: PHIHOX
Ranger result

Call:
 ranger(formulaString.lst[[j]], data = dfs, importance = "impurity",      write.forest = TRUE, mtry = t.mrfX$bestTune$mtry) 

Type:                             Regression 
Number of trees:                  500 
Sample size:                      604019 
Number of independent variables:  157 
Mtry:                             60 
Target node size:                 5 
Variable importance mode:         impurity 
OOB prediction error:             0.2516392 
R squared:                        0.8592144 

 Variable importance:
             [,1]
PRSMRG3 166941.25
DEPTH.f  68921.19
P07MRG3  63232.78
P10MRG3  41207.80
M04MOD4  31958.25
M05MOD4  24453.46
I05MOD4  20597.82
M10MOD4  20426.55
M09MOD4  16116.31
T07MOD3  16057.98
I09MOD4  15148.77
M11MOD4  13991.37
P03MRG3  11284.76
T06MOD3  10423.68
DEMMRG5  10257.55

eXtreme Gradient Boosting 

604019 samples
   157 predictor

No pre-processing
Resampling: Cross-Validated (3 fold, repeated 1 times) 
Summary of sample sizes: 402679, 402679, 402680 
Resampling results across tuning parameters:

  eta  max_depth  nrounds  RMSE       Rsquared 
  0.3  2           50      0.9178361  0.5290366
  0.3  2          100      0.8934964  0.5536811
  0.3  3           50      0.8839535  0.5632715
  0.3  3          100      0.8573668  0.5891095
  0.4  2           50      0.9116231  0.5352131
  0.4  2          100      0.8877636  0.5592398
  0.4  3           50      0.8769135  0.5698992
  0.4  3          100      0.8513619  0.5946709

Tuning parameter 'gamma' was held constant at a value of 0
Tuning
 parameter 'colsample_bytree' was held constant at a value of 0.8

Tuning parameter 'min_child_weight' was held constant at a value of 1
RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nrounds = 100, max_depth = 3, eta
 = 0.4, gamma = 0, colsample_bytree = 0.8 and min_child_weight = 1. 

 XGBoost variable importance:
    Feature        Gain       Cover   Frequence
 1: PRSMRG3 0.348309992 0.029247628 0.031609195
 2: P07MRG3 0.114933893 0.014030237 0.010057471
 3: M10MOD4 0.069683011 0.017789165 0.011494253
 4: I05MOD4 0.049735783 0.011179873 0.011494253
 5: M04MOD4 0.044560393 0.021315645 0.012931034
 6: DEPTH.f 0.038312920 0.054013299 0.053160920
 7: T05MOD3 0.025857365 0.014116360 0.010057471
 8: C02GLC5 0.023980951 0.005688559 0.004310345
 9: I08MOD4 0.023092816 0.010000004 0.004310345
10: VBFMRG5 0.014743934 0.023218323 0.033045977
11: DEMMRG5 0.012645292 0.024464543 0.031609195
12: L10USG5 0.010994294 0.001466323 0.002873563
13: I09MOD4 0.010879260 0.010214638 0.008620690
14: M08MOD4 0.009468596 0.014079728 0.012931034
15: VDPMRG5 0.009003027 0.019362709 0.015804598
--------------------------------------

Variable: PHIKCL
Ranger result

Call:
 ranger(formulaString.lst[[j]], data = dfs, importance = "impurity",      write.forest = TRUE, mtry = t.mrfX$bestTune$mtry) 

Type:                             Regression 
Number of trees:                  500 
Sample size:                      156437 
Number of independent variables:  157 
Mtry:                             55 
Target node size:                 5 
Variable importance mode:         impurity 
OOB prediction error:             0.30019 
R squared:                        0.8059904 

 Variable importance:
             [,1]
DEPTH.f 16614.111
I07MOD4 11922.915
VW2MOD1  9022.844
PRSMRG3  7725.310
I09MOD4  6464.096
C02GLC5  6428.880
I08MOD4  5946.253
VW1MOD1  5338.139
P07MRG3  4675.619
DEMMRG5  4455.783
M09MOD4  3792.524
EX1MOD5  3548.493
VW5MOD1  3452.577
VW6MOD1  2908.834
VDPMRG5  2781.403

eXtreme Gradient Boosting 

156437 samples
   157 predictor

No pre-processing
Resampling: Cross-Validated (3 fold, repeated 1 times) 
Summary of sample sizes: 104292, 104291, 104291 
Resampling results across tuning parameters:

  eta  max_depth  nrounds  RMSE       Rsquared 
  0.3  2           50      0.9396879  0.4310249
  0.3  2          100      0.9122446  0.4634076
  0.3  3           50      0.8998470  0.4783071
  0.3  3          100      0.8686120  0.5136247
  0.4  2           50      0.9304841  0.4414139
  0.4  2          100      0.9023054  0.4746739
  0.4  3           50      0.8903576  0.4883526
  0.4  3          100      0.8576294  0.5253417

Tuning parameter 'gamma' was held constant at a value of 0
Tuning
 parameter 'colsample_bytree' was held constant at a value of 0.8

Tuning parameter 'min_child_weight' was held constant at a value of 1
RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nrounds = 100, max_depth = 3, eta
 = 0.4, gamma = 0, colsample_bytree = 0.8 and min_child_weight = 1. 

 XGBoost variable importance:
    Feature       Gain       Cover   Frequence
 1: I07MOD4 0.12473802 0.017062302 0.008759124
 2: VW2MOD1 0.08840226 0.013985543 0.014598540
 3: PRSMRG3 0.07436275 0.020318750 0.021897810
 4: C02GLC5 0.05945125 0.015133231 0.010218978
 5: P02MRG3 0.05150215 0.009526838 0.007299270
 6: DEMMRG5 0.05147433 0.020541098 0.030656934
 7: DEPTH.f 0.03928556 0.054179816 0.065693431
 8: M09MOD4 0.03779971 0.034530839 0.023357664
 9: EX1MOD5 0.03376923 0.017028379 0.014598540
10: L05USG5 0.02257381 0.007785256 0.004379562
11: P06MRG3 0.02070531 0.014708479 0.017518248
12: P07MRG3 0.02008418 0.010485993 0.007299270
13: T07MOD3 0.01551444 0.008987215 0.005839416
14: VW4MOD1 0.01376406 0.015297281 0.016058394
15: T08MOD3 0.01349912 0.002224420 0.005839416
--------------------------------------

Variable: CRFVOL
Ranger result

Call:
 ranger(formulaString.lst[[j]], data = dfs, importance = "impurity",      write.forest = TRUE, mtry = t.mrfX$bestTune$mtry) 

Type:                             Regression 
Number of trees:                  500 
Sample size:                      303139 
Number of independent variables:  157 
Mtry:                             60 
Target node size:                 5 
Variable importance mode:         impurity 
OOB prediction error:             101.1627 
R squared:                        0.6252112 

 Variable importance:
              [,1]
DEPTH.f 10572966.7
TWIMRG5  3162194.4
VW3MOD1  2482303.8
DEMMRG5  2178630.9
VBFMRG5  1231719.2
ASSDAC3  1111233.0
VW5MOD1  1052258.8
VW2MOD1   859222.5
SLPMRG5   842603.2
VW4MOD1   828124.9
ES3MOD5   795119.2
VDPMRG5   779064.2
ES1MOD5   765554.9
ES4MOD5   757644.2
GTDHYS3   750549.5

eXtreme Gradient Boosting 

303139 samples
   157 predictor

No pre-processing
Resampling: Cross-Validated (3 fold, repeated 1 times) 
Summary of sample sizes: 202094, 202092, 202092 
Resampling results across tuning parameters:

  eta  max_depth  nrounds  RMSE      Rsquared 
  0.3  2           50      14.25206  0.2496592
  0.3  2          100      14.02710  0.2728225
  0.3  3           50      13.83234  0.2934189
  0.3  3          100      13.52316  0.3248563
  0.4  2           50      14.21233  0.2525083
  0.4  2          100      13.96944  0.2779407
  0.4  3           50      13.76600  0.2991049
  0.4  3          100      13.44188  0.3320024

Tuning parameter 'gamma' was held constant at a value of 0
Tuning
 parameter 'colsample_bytree' was held constant at a value of 0.8

Tuning parameter 'min_child_weight' was held constant at a value of 1
RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nrounds = 100, max_depth = 3, eta
 = 0.4, gamma = 0, colsample_bytree = 0.8 and min_child_weight = 1. 

 XGBoost variable importance:
    Feature       Gain       Cover   Frequence
 1: VW3MOD1 0.16489417 0.035150343 0.026162791
 2: DEPTH.f 0.08123370 0.046361624 0.059593023
 3: TWIMRG5 0.07717065 0.010770053 0.020348837
 4: T05MOD3 0.06931105 0.019741556 0.014534884
 5: VBFMRG5 0.06152648 0.025255281 0.020348837
 6: I07MOD4 0.03256067 0.010271700 0.008720930
 7: ASSDAC3 0.03021109 0.008440261 0.011627907
 8: DEMMRG5 0.02322806 0.036095708 0.042151163
 9: GTDHYS3 0.02266425 0.016878642 0.011627907
10: M07MOD4 0.02089312 0.006394784 0.007267442
11: T02MOD3 0.01776948 0.002770613 0.004360465
12: P06MRG3 0.01561561 0.012322741 0.013081395
13: ES4MOD5 0.01343983 0.019948810 0.015988372
14: VW1MOD1 0.01334654 0.008940187 0.008720930
15: P01MRG3 0.01333719 0.014888299 0.015988372
--------------------------------------

Variable: SNDPPT
Ranger result

Call:
 ranger(formulaString.lst[[j]], data = dfs, importance = "impurity",      write.forest = TRUE, mtry = t.mrfX$bestTune$mtry) 

Type:                             Regression 
Number of trees:                  500 
Sample size:                      616762 
Number of independent variables:  157 
Mtry:                             60 
Target node size:                 5 
Variable importance mode:         impurity 
OOB prediction error:             149.4583 
R squared:                        0.8136311 

 Variable importance:
            [,1]
DEPTH.f 31107974
P04MRG3 16585827
VW4MOD1 13857181
VBFMRG5  9569049
P11MRG3  9510586
VDPMRG5  9258948
VW1MOD1  8909622
DEMMRG5  8593343
T02MSD3  8446124
TWIMRG5  7080097
T11MSD3  6470986
VW2MOD1  6420628
VW3MOD1  5790718
N05MSD3  5689607
VW6MOD1  5673935

eXtreme Gradient Boosting 

616762 samples
   157 predictor

No pre-processing
Resampling: Cross-Validated (3 fold, repeated 1 times) 
Summary of sample sizes: 411174, 411175, 411175 
Resampling results across tuning parameters:

  eta  max_depth  nrounds  RMSE      Rsquared 
  0.3  2           50      23.66621  0.3064013
  0.3  2          100      22.98828  0.3452502
  0.3  3           50      22.61770  0.3665082
  0.3  3          100      21.87411  0.4063941
  0.4  2           50      23.46592  0.3161743
  0.4  2          100      22.77651  0.3551969
  0.4  3           50      22.41886  0.3757880
  0.4  3          100      21.64021  0.4174810

Tuning parameter 'gamma' was held constant at a value of 0
Tuning
 parameter 'colsample_bytree' was held constant at a value of 0.8

Tuning parameter 'min_child_weight' was held constant at a value of 1
RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nrounds = 100, max_depth = 3, eta
 = 0.4, gamma = 0, colsample_bytree = 0.8 and min_child_weight = 1. 

 XGBoost variable importance:
    Feature       Gain       Cover   Frequence
 1: P11MRG3 0.10086960 0.012403495 0.007173601
 2: T02MSD3 0.06481966 0.009504489 0.011477762
 3: P04MRG3 0.06165777 0.013182958 0.014347202
 4: VBFMRG5 0.04452525 0.013795199 0.028694405
 5: VW2MOD1 0.03971697 0.015025858 0.014347202
 6: VW4MOD1 0.03902392 0.031117745 0.024390244
 7: VDPMRG5 0.03483977 0.011055541 0.022955524
 8: I09MOD4 0.03333897 0.012241677 0.011477762
 9: N02MOD3 0.02622034 0.008754653 0.005738881
10: T09MOD3 0.02518753 0.008507108 0.007173601
11: VW1MOD1 0.02501037 0.018002576 0.021520803
12: N09MSD3 0.02211432 0.009412752 0.004304161
13: DEMMRG5 0.01885465 0.018022043 0.025824964
14: M11MOD4 0.01759401 0.006699614 0.007173601
15: C01GLC5 0.01701898 0.012469409 0.008608321
--------------------------------------

Variable: SLTPPT
Ranger result

Call:
 ranger(formulaString.lst[[j]], data = dfs, importance = "impurity",      write.forest = TRUE, mtry = t.mrfX$bestTune$mtry) 

Type:                             Regression 
Number of trees:                  500 
Sample size:                      613750 
Number of independent variables:  157 
Mtry:                             55 
Target node size:                 5 
Variable importance mode:         impurity 
OOB prediction error:             85.2571 
R squared:                        0.8165339 

 Variable importance:
            [,1]
N02MOD3 20150314
DEPTH.f 18962420
N12MOD3 11869392
T12MOD3 11150404
P04MRG3  7456897
T01MOD3  7102961
VW4MOD1  7008262
VBFMRG5  4846548
VDPMRG5  4257860
N03MOD3  3944558
N01MOD3  3876201
DEMMRG5  3820597
TWIMRG5  3225316
VW3MOD1  3036245
T11MSD3  2800215

eXtreme Gradient Boosting 

613750 samples
   157 predictor

No pre-processing
Resampling: Cross-Validated (3 fold, repeated 1 times) 
Summary of sample sizes: 409166, 409167, 409167 
Resampling results across tuning parameters:

  eta  max_depth  nrounds  RMSE      Rsquared 
  0.3  2           50      16.65763  0.4053287
  0.3  2          100      16.14661  0.4412978
  0.3  3           50      15.83234  0.4628079
  0.3  3          100      15.34562  0.4948748
  0.4  2           50      16.45618  0.4189641
  0.4  2          100      15.94688  0.4540882
  0.4  3           50      15.68389  0.4718979
  0.4  3          100      15.17487  0.5052760

Tuning parameter 'gamma' was held constant at a value of 0
Tuning
 parameter 'colsample_bytree' was held constant at a value of 0.8

Tuning parameter 'min_child_weight' was held constant at a value of 1
RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nrounds = 100, max_depth = 3, eta
 = 0.4, gamma = 0, colsample_bytree = 0.8 and min_child_weight = 1. 

 XGBoost variable importance:
    Feature       Gain       Cover   Frequence
 1: N02MOD3 0.24516626 0.008690335 0.005714286
 2: T02MOD3 0.09451886 0.008982669 0.008571429
 3: P04MRG3 0.09128855 0.016383489 0.017142857
 4: T01MOD3 0.06225980 0.009108448 0.010000000
 5: DEPTH.f 0.02965121 0.041434086 0.032857143
 6: DEMMRG5 0.02795758 0.022335283 0.032857143
 7: VBFMRG5 0.02544171 0.023071381 0.030000000
 8: N06MOD3 0.02522274 0.011307400 0.010000000
 9: TWIMRG5 0.02009692 0.009917529 0.012857143
10: VW1MOD1 0.01769586 0.019016060 0.014285714
11: M03MOD4 0.01504597 0.017599299 0.015714286
12: P08MRG3 0.01420730 0.011816587 0.014285714
13: VDPMRG5 0.01398536 0.007472136 0.020000000
14: M01MOD4 0.01377181 0.008149198 0.005714286
15: ASSDAC3 0.01222789 0.021109274 0.022857143
--------------------------------------

Variable: CLYPPT
Ranger result

Call:
 ranger(formulaString.lst[[j]], data = dfs, importance = "impurity",      write.forest = TRUE, mtry = t.mrfX$bestTune$mtry) 

Type:                             Regression 
Number of trees:                  500 
Sample size:                      625159 
Number of independent variables:  157 
Mtry:                             35 
Target node size:                 5 
Variable importance mode:         impurity 
OOB prediction error:             84.10557 
R squared:                        0.7472457 

 Variable importance:
            [,1]
DEPTH.f 15743638
T10MOD3  3776262
DEMMRG5  3430345
T09MOD3  3325627
VBFMRG5  3064739
T11MOD3  2776244
T04MOD3  2716873
VW1MOD1  2660792
VW4MOD1  2649248
T01MOD3  2547931
TWIMRG5  2456330
VW5MOD1  2256806
VDPMRG5  2118860
VW2MOD1  2088896
M11MOD4  2031871

eXtreme Gradient Boosting 

625159 samples
   157 predictor

No pre-processing
Resampling: Cross-Validated (3 fold, repeated 1 times) 
Summary of sample sizes: 416774, 416772, 416772 
Resampling results across tuning parameters:

  eta  max_depth  nrounds  RMSE      Rsquared 
  0.3  2           50      15.70992  0.2645682
  0.3  2          100      15.32472  0.2988538
  0.3  3           50      15.11458  0.3182235
  0.3  3          100      14.66885  0.3567997
  0.4  2           50      15.58718  0.2739401
  0.4  2          100      15.18936  0.3092059
  0.4  3           50      14.99112  0.3272121
  0.4  3          100      14.51489  0.3685618

Tuning parameter 'gamma' was held constant at a value of 0
Tuning
 parameter 'colsample_bytree' was held constant at a value of 0.8

Tuning parameter 'min_child_weight' was held constant at a value of 1
RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nrounds = 100, max_depth = 3, eta
 = 0.4, gamma = 0, colsample_bytree = 0.8 and min_child_weight = 1. 

 XGBoost variable importance:
    Feature       Gain       Cover   Frequence
 1: T10MOD3 0.11561453 0.009217385 0.010028653
 2: DEPTH.f 0.08118609 0.054607288 0.051575931
 3: DEMMRG5 0.04699080 0.030589563 0.035816619
 4: T09MOD3 0.04684673 0.008776233 0.004297994
 5: T12MOD3 0.04237294 0.017192094 0.011461318
 6: VBFMRG5 0.03273777 0.009359418 0.015759312
 7: VW6MOD1 0.02839248 0.006892046 0.011461318
 8: VW5MOD1 0.02516321 0.018943052 0.018624642
 9: VW4MOD1 0.02509433 0.019458526 0.031518625
10: M11MOD4 0.02448245 0.004868549 0.005730659
11: SN2MOD4 0.02314152 0.009881739 0.008595989
12: P04MRG3 0.02286019 0.013046582 0.010028653
13: T05MOD3 0.02001676 0.005844498 0.005730659
14: T11MSD3 0.01940214 0.007454772 0.007163324
15: P01MRG3 0.01767614 0.012104091 0.012893983
--------------------------------------

Variable: BLD
Ranger result

Call:
 ranger(formulaString.lst[[j]], data = dfs, importance = "impurity",      write.forest = TRUE, mtry = t.mrfX$bestTune$mtry) 

Type:                             Regression 
Number of trees:                  500 
Sample size:                      140596 
Number of independent variables:  157 
Mtry:                             60 
Target node size:                 5 
Variable importance mode:         impurity 
OOB prediction error:             23495.5 
R squared:                        0.7901882 

 Variable importance:
              [,1]
DEPTH.f 2578117486
M03MOD4  822724550
M04MOD4  434633845
T09MOD3  409298159
T05MOD3  324980547
TWIMRG5  291653624
ES2MOD5  228191458
DEMMRG5  224770128
T02MOD3  224568017
VW6MOD1  192680585
T04MOD3  192099060
VDPMRG5  186945303
VW1MOD1  172821669
T10MOD3  168096381
SN2MOD4  163852253

eXtreme Gradient Boosting 

140596 samples
   157 predictor

No pre-processing
Resampling: Cross-Validated (3 fold, repeated 1 times) 
Summary of sample sizes: 93730, 93731, 93731 
Resampling results across tuning parameters:

  eta  max_depth  nrounds  RMSE      Rsquared 
  0.3  2           50      234.1743  0.5126522
  0.3  2          100      225.8408  0.5456639
  0.3  3           50      221.6516  0.5627571
  0.3  3          100      212.3756  0.5980564
  0.4  2           50      231.3901  0.5228513
  0.4  2          100      223.0649  0.5563456
  0.4  3           50      219.8359  0.5688318
  0.4  3          100      210.5739  0.6042955

Tuning parameter 'gamma' was held constant at a value of 0
Tuning
 parameter 'colsample_bytree' was held constant at a value of 0.8

Tuning parameter 'min_child_weight' was held constant at a value of 1
RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nrounds = 100, max_depth = 3, eta
 = 0.4, gamma = 0, colsample_bytree = 0.8 and min_child_weight = 1. 

 XGBoost variable importance:
    Feature       Gain       Cover   Frequence
 1: DEPTH.f 0.21549076 0.051474474 0.065312046
 2: M03MOD4 0.12984323 0.024137530 0.021770682
 3: T05MOD3 0.05670985 0.009171958 0.007256894
 4: VDPMRG5 0.03847334 0.018255126 0.034833091
 5: P05MRG3 0.03355614 0.006396218 0.010159652
 6: NEGMRG5 0.03148105 0.001528023 0.002902758
 7: N07MOD3 0.02896750 0.014719026 0.007256894
 8: SN4MOD4 0.02707161 0.015599851 0.011611030
 9: T02MOD3 0.02662489 0.004042730 0.005805515
10: VW6MOD1 0.02435274 0.018979260 0.015965167
11: ES2MOD5 0.02221876 0.010694125 0.007256894
12: VW4MOD1 0.01710564 0.027170935 0.020319303
13: DEMMRG5 0.01615480 0.023558375 0.037735849
14: L09USG5 0.01354727 0.006666687 0.002902758
15: N11MSD3 0.01171392 0.010637272 0.013062409
--------------------------------------

Variable: CECSUM
Ranger result

Call:
 ranger(formulaString.lst[[j]], data = dfs, importance = "impurity",      write.forest = TRUE, mtry = t.mrfX$bestTune$mtry) 

Type:                             Regression 
Number of trees:                  500 
Sample size:                      393585 
Number of independent variables:  157 
Mtry:                             50 
Target node size:                 5 
Variable importance mode:         impurity 
OOB prediction error:             97.55433 
R squared:                        0.6746771 

 Variable importance:
           [,1]
DEPTH.f 9619565
N11MSD3 2092305
L04USG5 2026838
VW1MOD1 1823798
N09MSD3 1736210
DEMMRG5 1445695
TWIMRG5 1353357
VDPMRG5 1281069
N06MSD3 1273826
SN1MOD4 1223482
T02MSD3 1214462
T12MOD3 1211368
N02MOD3 1196401
VBFMRG5 1179085
VW4MOD1 1120422

eXtreme Gradient Boosting 

393585 samples
   157 predictor

No pre-processing
Resampling: Cross-Validated (3 fold, repeated 1 times) 
Summary of sample sizes: 262391, 262390, 262389 
Resampling results across tuning parameters:

  eta  max_depth  nrounds  RMSE      Rsquared 
  0.3  2           50      15.03908  0.2546782
  0.3  2          100      14.63946  0.2899340
  0.3  3           50      14.45360  0.3093089
  0.3  3          100      14.03311  0.3467457
  0.4  2           50      14.91511  0.2628900
  0.4  2          100      14.52387  0.2987294
  0.4  3           50      14.29110  0.3214853
  0.4  3          100      13.89301  0.3578727

Tuning parameter 'gamma' was held constant at a value of 0
Tuning
 parameter 'colsample_bytree' was held constant at a value of 0.8

Tuning parameter 'min_child_weight' was held constant at a value of 1
RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nrounds = 100, max_depth = 3, eta
 = 0.4, gamma = 0, colsample_bytree = 0.8 and min_child_weight = 1. 

 XGBoost variable importance:
    Feature       Gain        Cover   Frequence
 1: N11MSD3 0.04932282 0.0162516459 0.008708273
 2: N09MSD3 0.04784970 0.0164310059 0.017416546
 3: DEPTH.f 0.04539764 0.0497268250 0.050798258
 4: L04USG5 0.04493128 0.0082384967 0.004354136
 5: N02MOD3 0.03858638 0.0023359645 0.007256894
 6: VW1MOD1 0.03759282 0.0116211142 0.018867925
 7: P01MRG3 0.03399100 0.0177478167 0.014513788
 8: SN1MOD4 0.02842926 0.0185307908 0.015965167
 9: L09USG5 0.02823953 0.0083747317 0.008708273
10: T03MOD3 0.02568445 0.0160725062 0.011611030
11: M03MOD4 0.02474858 0.0059194367 0.010159652
12: VDPMRG5 0.02133001 0.0114414325 0.015965167
13: DEMMRG5 0.02017296 0.0260866306 0.037735849
14: VW2MOD1 0.01914848 0.0003263931 0.005805515
15: TMDMOD3 0.01851224 0.0091857726 0.007256894
--------------------------------------
